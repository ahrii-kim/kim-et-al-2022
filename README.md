# The Suboptimal WMT Test Sets and Their Impact on Human Parity
<img src="https://img.shields.io/badge/Python-3766AB?style=flat-square&logo=Python&logoColor=white"/></a>



## About
This repository provides codes and datasets of the NMT evaluation for the following publication "The Suboptimal WMT Test Sets and Their Impact on Human Parity" by Ahrii Kim, Yunju Bak, Jimin Sun, Sungwon Lyu, Changmin Lee, submitted at Preprints.    

a part of WMT 2020 English-III test set translated into Korean

With the advent of Neural Machine Translation, the more the achievement of human-machine parity is claimed at WMT, the more we come to ask ourselves if their evaluation environment can be trusted. In this paper, we argue that the low quality of the source test set of the news track at WMT may lead to an overrated human parity claim.

First of all, we report nine types of so-called \textit{technical contaminants} in the data set, originated from an absence of meticulous inspection after web-crawling. Our empirical findings show that when they are corrected, about 5\% of the segments that have previously achieved a human parity claim turn out to be statistically invalid. Such a tendency gets evident when the contaminated sentences are solely concerned. 

To the best of our knowledge, it is the first attempt to question the ``source'' side of the test set as a potential cause of the overclaim of human parity. We cast evidence for such phenomenon that according to sentence-level TER scores, those trivial errors change a good part of system translations. We conclude that to overlook it would be a mistake, especially when it comes to an NMT evaluation.

### Paper
Bibtex:
```sh
   @article
```
 
 
 ### Contact
 For more questions, please contact the first author of the paper.
 
 ### License
 
  
  

